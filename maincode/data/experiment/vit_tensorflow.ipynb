{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############### 필요 모듈 로드 #################\n",
    "###############################################\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.activations import gelu\n",
    "import tensorflow_addons as tfa\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "###############################################\n",
    "############ 멀티 헤드 어텐션 정의 ##############\n",
    "###############################################\n",
    " \n",
    "class MultiHeadedAttention(tf.keras.Model):\n",
    "    def __init__(self, dimension: int, heads: int = 8):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.dimension = dimension\n",
    "        assert dimension // heads\n",
    "        self.depth = dimension // heads\n",
    "        self.wq = tf.keras.layers.Dense(dimension)\n",
    "        self.wk = tf.keras.layers.Dense(dimension)\n",
    "        self.wv = tf.keras.layers.Dense(dimension)\n",
    "        self.dense = tf.keras.layers.Dense(dimension)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        q: tf.Tensor = self.wq(inputs)\n",
    "        k: tf.Tensor = self.wk(inputs)\n",
    "        v: tf.Tensor = self.wv(inputs)\n",
    " \n",
    "        def split_heads(x, batch_size):\n",
    "            x = tf.reshape(x, (batch_size, -1, self.heads, self.depth))\n",
    "            return tf.transpose(x, perm=[0,2,1,3])\n",
    " \n",
    "        q = split_heads(q, batch_size)\n",
    "        k = split_heads(k, batch_size)\n",
    "        v = split_heads(v, batch_size)\n",
    " \n",
    "        def scaled_dot_product_attention(q,k,v):\n",
    "            matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "            dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "            scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    " \n",
    "            softmax = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "            scaled_dot_product_attention_output = tf.matmul(softmax, v)\n",
    "            return scaled_dot_product_attention_output, softmax\n",
    "        \n",
    "        attention_weights, softmax = scaled_dot_product_attention(q, k, v)\n",
    "        scaled_attention = tf.transpose(attention_weights, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.dimension))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "############ 레지듀얼 블록 정의 ################\n",
    "###############################################\n",
    " \n",
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, residual_function):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.residual_function = residual_function\n",
    " \n",
    "    def call(self, inputs):\n",
    "        return self.residual_function(inputs) + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " ###############################################\n",
    "######## LayerNormalization 정의 ##############\n",
    "###############################################\n",
    " \n",
    "class NormalizationBlock(tf.keras.Model):\n",
    "    def __init__(self, norm_function, epsilon=1e-5):\n",
    "        super(NormalizationBlock, self).__init__()\n",
    "        self.norm_function = norm_function\n",
    "        self.normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        return self.norm_function(self.normalize(inputs))\n",
    " \n",
    "###############################################\n",
    "############### MLP 블록 정의 #################\n",
    "###############################################\n",
    " \n",
    "class MLPBlock(tf.keras.Model):\n",
    "    def __init__(self, output_dimension, hidden_dimension):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.output_dimension = tf.keras.layers.Dense(output_dimension)\n",
    "        self.hidden_dimension = tf.keras.layers.Dense(hidden_dimension)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        x = self.hidden_dimension(inputs)\n",
    "        x = gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.output_dimension(x)\n",
    "        x = gelu(x)\n",
    "        output = self.dropout2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############ 트랜스포머 인코더 정의 #############\n",
    "###############################################\n",
    " \n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dimension, depth, heads, mlp_dimension): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers_ = []\n",
    "        layers_.append(tf.keras.Input(shape=((CFG.obj_image_size//CFG.patch_size)*(CFG.obj_image_size//CFG.patch_size)+1,dimension)))\n",
    "        for i in range(depth):\n",
    "            layers_.append(NormalizationBlock(ResidualBlock(MultiHeadedAttention(dimension, heads))))\n",
    "            layers_.append(NormalizationBlock(ResidualBlock(MLPBlock(dimension, mlp_dimension))))\n",
    "        \n",
    "        self.layers_ = tf.keras.Sequential(layers_)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        return self.layers_(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "###############################################\n",
    "############### VIT 전체 구현 #################\n",
    "###############################################\n",
    " \n",
    "class ImageTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "            self, image_size, patch_size, n_classes, batch_size,\n",
    "            dimension, depth, heads, mlp_dimension, channels=3):\n",
    "        super(ImageTransformer, self).__init__()\n",
    "        assert image_size % patch_size == 0, 'invalid patch size for image size'\n",
    " \n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.dimension = dimension\n",
    "        self.batch_size = batch_size\n",
    " \n",
    "        self.positional_embedding = self.add_weight(\n",
    "            \"position_embeddings\", shape=[num_patches + 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        self.classification_token = self.add_weight(\n",
    "            \"classification_token\", shape=[1, 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        self.heads = heads\n",
    "        self.depth = depth\n",
    "        self.mlp_dimension = dimension\n",
    "        self.n_classes = n_classes\n",
    "        self.num_patches = num_patches\n",
    " \n",
    "        self.patch_projection = tf.keras.layers.Dense(dimension)\n",
    "        self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.MLP = MLPBlock(self.dimension, self.mlp_dimension)\n",
    "        self.output_classes = tf.keras.layers.Dense(self.n_classes)\n",
    "        self.transformer = TransformerEncoder(self.dimension, self.depth, self.heads, self.mlp_dimension)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.5)\n",
    " \n",
    "    def call(self, inputs):       \n",
    "        output = None\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        ###############################################\n",
    "        ############ 가장 중요한 부분 ##################\n",
    "        ###############################################\n",
    " \n",
    "        # 이미지를 patch_size로 조각낸다.\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = inputs,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1,1,1,1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    " \n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, patches.shape[1]*patches.shape[2], patch_dims])\n",
    "        x = self.patch_projection(patches)\n",
    "        \n",
    "        cls_pos = tf.broadcast_to(\n",
    "            self.classification_token, [batch_size, 1, self.dimension]\n",
    "        )\n",
    "        x = tf.concat([cls_pos, x], axis=1)\n",
    "        x = x + self.positional_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = self.normalization2(x)\n",
    "        x = x[:,0,:]\n",
    "        x_keep = tf.identity(x)\n",
    "        x = self.dropout1(x)\n",
    "        output = self.output_classes(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_classes = 10\n",
    "    input_shape = (32, 32, 3)\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    batch_size = 256\n",
    "    num_epochs = 100\n",
    "    image_size = 32\n",
    "    obj_image_size = 32\n",
    "    patch_size = 4\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    projection_dim = 64\n",
    "    num_heads = 8\n",
    "    transformer_layers = 2\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = CFG()\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate = CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    " \n",
    "model_vit = ImageTransformer(CFG.image_size, CFG.patch_size, CFG.num_classes, CFG.batch_size, CFG.projection_dim, CFG.transformer_layers, CFG.num_heads, CFG.projection_dim)\n",
    "model_vit.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
    " \n",
    "model_vit.fit(x=train_images, y=train_labels, batch_size=CFG.batch_size, epochs=CFG.num_epochs, validation_data=(test_images, test_labels), shuffle=True)\n",
    "print('==============Training Finished===============')\n",
    " \n",
    "accuracy = 0\n",
    "_, accuracy = model_vit.evaluate(test_images, test_labels)\n",
    " \n",
    "print('Test Accuracy :', accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
