{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d6a4e3-93b4-4998-b999-56adc65deaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave\\anaconda3\\envs\\haikoo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6d1a8d-34d2-415f-95e5-f82d50944f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = r'C:../dataset/audioonly/labeled/original_dataset'\n",
    "os.path.exists(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cfa4d0-b0ec-45c2-a29a-21ed49051db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belly_pain', 'discomfort', 'hungry', 'tired']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = next(os.walk(dir_path))[1]\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf011ed-a8a0-4a29-8eee-d75f973514af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belly_pain', 'discomfort', 'hungry', 'tired']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a180cc-bccd-44c4-800d-160ca723d1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:../dataset/audioonly/labeled/original_dataset\\\\belly_pain',\n",
       " 'C:../dataset/audioonly/labeled/original_dataset\\\\discomfort',\n",
       " 'C:../dataset/audioonly/labeled/original_dataset\\\\hungry',\n",
       " 'C:../dataset/audioonly/labeled/original_dataset\\\\tired']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dir = [os.path.join(dir_path, name) for name in class_name]\n",
    "audio_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65bc4009-56c0-4540-8d9a-34a84ebbe49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpain_audio = glob.glob(os.path.join(audio_dir[0], '*.wav'))\n",
    "discomf_audio = glob.glob(os.path.join(audio_dir[1], '*.wav'))\n",
    "hungry_audio = glob.glob(os.path.join(audio_dir[2], '*.wav'))\n",
    "tired_audio = glob.glob(os.path.join(audio_dir[3], '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fb22f0-b97c-4c7c-9205-cc184be30da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path_class = {\n",
    "    'bpain': bpain_audio,\n",
    "    'discomf': discomf_audio,\n",
    "    'hungry': hungry_audio,\n",
    "    'tired': tired_audio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3eb7d3d-79d9-469e-b0f8-c115b8037a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetWithSubDirs(Dataset):\n",
    "    def __init__(self, root_dir, sr=16000, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sr = sr\n",
    "        self.audio_labels = []\n",
    "        self.audio_paths = []\n",
    "        self.class_name = next(os.walk(self.root_dir))[1]\n",
    "        self.processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        for name in self.class_name:\n",
    "            if name == '.ipynb_checkpoints':\n",
    "                self.class_name.remove(name)\n",
    "\n",
    "        self.audio_subdir = [os.path.join(dir_path, name) for name in class_name]\n",
    "\n",
    "        for dir, (index, name) in zip(self.audio_subdir, enumerate(self.class_name)):\n",
    "            temp_paths = glob.glob(os.path.join(dir, '*.wav'))\n",
    "            for file in temp_paths:\n",
    "                self.audio_paths.append(file)\n",
    "                self.audio_labels.append(index)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.audio_labels[idx]\n",
    "        waveform, _ = librosa.load(audio_path, sr=self.sr, duration=4.9)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform, sample_rate=self.sr)\n",
    "            waveform = self.processor(waveform, sampling_rate=self.sr, return_tensor='pt')\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27c43f6-d158-43aa-a39d-dd92173b0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "])\n",
    "\n",
    "audio_dataset = AudioDatasetWithSubDirs(root_dir=dir_path, sr=16000, transform=augmentations)\n",
    "audio_dataloader = DataLoader(audio_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d541d544-ae2d-4eea-b0af-9c87a6e5fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 2, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 0, 2, 2, 2, 3, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 3, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 0, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 2, 2, 2, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 3, 0, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 1, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 1, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 3, 1, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 1, 1, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 3, 3, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 0, 1, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 0, 2, 1, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 0, 2, 1, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 0, 2, 1, 2, 3])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 0, 2, 2, 1, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 1, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 0, 2, 3, 0, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 0, 2, 3, 2, 1, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 0, 0, 1, 2, 2, 2, 3])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 1, 2, 2, 0, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 1, 0, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 3, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 1, 2, 2, 3, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 1, 1, 2, 2, 0, 3, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 0, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 1, 2, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 0, 2, 2, 2, 2, 0, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 3, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 2, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 3, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 1, 1, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 3, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 3, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 3, 1, 2, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 3, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 1, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 2, 0, 2, 2, 3, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 1, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 1, 0, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 0, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 1, 0, 3, 1, 0, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 1, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 0, 2, 2, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 3, 2, 2, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 1, 2, 2, 2, 3, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 3, 2, 2, 2, 2, 0, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 1, 2, 2, 2, 1, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 2, 1, 1, 2, 2, 1])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 1, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([0, 2, 1, 2, 2, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([1, 2, 2, 2, 0, 2, 2, 2])\n",
      "torch.Size([8, 1024, 128])\n",
      "tensor([2, 0, 1, 0, 2, 3, 2, 2])\n",
      "torch.Size([2, 1024, 128])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "temp = []\n",
    "\n",
    "for waveform, label in audio_dataloader:\n",
    "    print(waveform['input_values'][0].shape)\n",
    "    print(label)\n",
    "    # temp.append(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5d6c7-0487-4e82-8f2b-dc1369f5cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "\n",
    "# for audio in temp[0][:5]:\n",
    "#     np_audio = audio.numpy()\n",
    "#     sound = Audio(np_audio, rate=16000)\n",
    "#     display(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ba8b0-f31d-49a7-9aaa-cf3c75e122eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haikoo",
   "language": "python",
   "name": "haikoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
